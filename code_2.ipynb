{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w</th>\n",
       "      <th>y</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7326.0</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>7326.000000</td>\n",
       "      <td>7326.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.079811</td>\n",
       "      <td>0.016903</td>\n",
       "      <td>0.018974</td>\n",
       "      <td>0.023204</td>\n",
       "      <td>0.027638</td>\n",
       "      <td>0.031068</td>\n",
       "      <td>0.039904</td>\n",
       "      <td>0.045646</td>\n",
       "      <td>0.051259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036622</td>\n",
       "      <td>0.039024</td>\n",
       "      <td>0.043605</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.049477</td>\n",
       "      <td>0.055759</td>\n",
       "      <td>0.060715</td>\n",
       "      <td>0.070436</td>\n",
       "      <td>0.078284</td>\n",
       "      <td>0.081028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.912495</td>\n",
       "      <td>0.525411</td>\n",
       "      <td>0.524915</td>\n",
       "      <td>0.528490</td>\n",
       "      <td>0.534649</td>\n",
       "      <td>0.538805</td>\n",
       "      <td>0.541786</td>\n",
       "      <td>0.548709</td>\n",
       "      <td>0.554192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.779625</td>\n",
       "      <td>0.791860</td>\n",
       "      <td>0.801430</td>\n",
       "      <td>0.813512</td>\n",
       "      <td>0.825543</td>\n",
       "      <td>0.832994</td>\n",
       "      <td>0.847803</td>\n",
       "      <td>0.862117</td>\n",
       "      <td>0.881227</td>\n",
       "      <td>0.893482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.911402</td>\n",
       "      <td>-1.262334</td>\n",
       "      <td>-1.272579</td>\n",
       "      <td>-1.274533</td>\n",
       "      <td>-1.263872</td>\n",
       "      <td>-1.330827</td>\n",
       "      <td>-1.283868</td>\n",
       "      <td>-1.358741</td>\n",
       "      <td>-1.286085</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.760937</td>\n",
       "      <td>-1.759061</td>\n",
       "      <td>-1.758994</td>\n",
       "      <td>-1.798526</td>\n",
       "      <td>-1.867828</td>\n",
       "      <td>-1.786994</td>\n",
       "      <td>-1.858266</td>\n",
       "      <td>-1.893545</td>\n",
       "      <td>-1.942976</td>\n",
       "      <td>-1.842185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.641770</td>\n",
       "      <td>-0.367108</td>\n",
       "      <td>-0.360759</td>\n",
       "      <td>-0.359213</td>\n",
       "      <td>-0.366740</td>\n",
       "      <td>-0.365066</td>\n",
       "      <td>-0.354121</td>\n",
       "      <td>-0.352320</td>\n",
       "      <td>-0.345838</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.559104</td>\n",
       "      <td>-0.557008</td>\n",
       "      <td>-0.557189</td>\n",
       "      <td>-0.571088</td>\n",
       "      <td>-0.584981</td>\n",
       "      <td>-0.580992</td>\n",
       "      <td>-0.599156</td>\n",
       "      <td>-0.605835</td>\n",
       "      <td>-0.608209</td>\n",
       "      <td>-0.630070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.154423</td>\n",
       "      <td>-0.003592</td>\n",
       "      <td>-0.001574</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>0.009823</td>\n",
       "      <td>0.017216</td>\n",
       "      <td>0.028255</td>\n",
       "      <td>0.034515</td>\n",
       "      <td>0.045895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069024</td>\n",
       "      <td>0.078731</td>\n",
       "      <td>0.086139</td>\n",
       "      <td>0.097776</td>\n",
       "      <td>0.114833</td>\n",
       "      <td>0.126746</td>\n",
       "      <td>0.117710</td>\n",
       "      <td>0.134059</td>\n",
       "      <td>0.152927</td>\n",
       "      <td>0.142485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.847007</td>\n",
       "      <td>0.384624</td>\n",
       "      <td>0.380424</td>\n",
       "      <td>0.386953</td>\n",
       "      <td>0.409558</td>\n",
       "      <td>0.426038</td>\n",
       "      <td>0.444288</td>\n",
       "      <td>0.444514</td>\n",
       "      <td>0.465356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633095</td>\n",
       "      <td>0.662089</td>\n",
       "      <td>0.669721</td>\n",
       "      <td>0.689635</td>\n",
       "      <td>0.703783</td>\n",
       "      <td>0.719244</td>\n",
       "      <td>0.749348</td>\n",
       "      <td>0.771733</td>\n",
       "      <td>0.808768</td>\n",
       "      <td>0.821933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.916584</td>\n",
       "      <td>1.385602</td>\n",
       "      <td>1.409508</td>\n",
       "      <td>1.423429</td>\n",
       "      <td>1.440747</td>\n",
       "      <td>1.391851</td>\n",
       "      <td>1.375212</td>\n",
       "      <td>1.409847</td>\n",
       "      <td>1.395246</td>\n",
       "      <td>...</td>\n",
       "      <td>1.772466</td>\n",
       "      <td>1.779313</td>\n",
       "      <td>1.735670</td>\n",
       "      <td>1.729854</td>\n",
       "      <td>1.763416</td>\n",
       "      <td>1.820125</td>\n",
       "      <td>1.872739</td>\n",
       "      <td>1.897289</td>\n",
       "      <td>1.929303</td>\n",
       "      <td>1.869503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            w            y            0            1            2  \\\n",
       "count  7326.0  7326.000000  7326.000000  7326.000000  7326.000000   \n",
       "mean      1.0     0.079811     0.016903     0.018974     0.023204   \n",
       "std       0.0     0.912495     0.525411     0.524915     0.528490   \n",
       "min       1.0    -1.911402    -1.262334    -1.272579    -1.274533   \n",
       "25%       1.0    -0.641770    -0.367108    -0.360759    -0.359213   \n",
       "50%       1.0     0.154423    -0.003592    -0.001574     0.002993   \n",
       "75%       1.0     0.847007     0.384624     0.380424     0.386953   \n",
       "max       1.0     1.916584     1.385602     1.409508     1.423429   \n",
       "\n",
       "                 3            4            5            6            7  ...  \\\n",
       "count  7326.000000  7326.000000  7326.000000  7326.000000  7326.000000  ...   \n",
       "mean      0.027638     0.031068     0.039904     0.045646     0.051259  ...   \n",
       "std       0.534649     0.538805     0.541786     0.548709     0.554192  ...   \n",
       "min      -1.263872    -1.330827    -1.283868    -1.358741    -1.286085  ...   \n",
       "25%      -0.366740    -0.365066    -0.354121    -0.352320    -0.345838  ...   \n",
       "50%       0.009823     0.017216     0.028255     0.034515     0.045895  ...   \n",
       "75%       0.409558     0.426038     0.444288     0.444514     0.465356  ...   \n",
       "max       1.440747     1.391851     1.375212     1.409847     1.395246  ...   \n",
       "\n",
       "                40           41           42           43           44  \\\n",
       "count  7326.000000  7326.000000  7326.000000  7326.000000  7326.000000   \n",
       "mean      0.036622     0.039024     0.043605     0.045400     0.049477   \n",
       "std       0.779625     0.791860     0.801430     0.813512     0.825543   \n",
       "min      -1.760937    -1.759061    -1.758994    -1.798526    -1.867828   \n",
       "25%      -0.559104    -0.557008    -0.557189    -0.571088    -0.584981   \n",
       "50%       0.069024     0.078731     0.086139     0.097776     0.114833   \n",
       "75%       0.633095     0.662089     0.669721     0.689635     0.703783   \n",
       "max       1.772466     1.779313     1.735670     1.729854     1.763416   \n",
       "\n",
       "                45           46           47           48           49  \n",
       "count  7326.000000  7326.000000  7326.000000  7326.000000  7326.000000  \n",
       "mean      0.055759     0.060715     0.070436     0.078284     0.081028  \n",
       "std       0.832994     0.847803     0.862117     0.881227     0.893482  \n",
       "min      -1.786994    -1.858266    -1.893545    -1.942976    -1.842185  \n",
       "25%      -0.580992    -0.599156    -0.605835    -0.608209    -0.630070  \n",
       "50%       0.126746     0.117710     0.134059     0.152927     0.142485  \n",
       "75%       0.719244     0.749348     0.771733     0.808768     0.821933  \n",
       "max       1.820125     1.872739     1.897289     1.929303     1.869503  \n",
       "\n",
       "[8 rows x 52 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.670595</td>\n",
       "      <td>-0.839068</td>\n",
       "      <td>-0.734415</td>\n",
       "      <td>-0.587261</td>\n",
       "      <td>-0.788800</td>\n",
       "      <td>-0.975857</td>\n",
       "      <td>-0.774088</td>\n",
       "      <td>-1.021334</td>\n",
       "      <td>-1.110286</td>\n",
       "      <td>-0.893337</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.997627</td>\n",
       "      <td>-1.103794</td>\n",
       "      <td>-1.092988</td>\n",
       "      <td>-0.989165</td>\n",
       "      <td>-0.827528</td>\n",
       "      <td>-0.813729</td>\n",
       "      <td>-0.532411</td>\n",
       "      <td>-0.289483</td>\n",
       "      <td>-0.407720</td>\n",
       "      <td>-0.407505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.188165</td>\n",
       "      <td>0.166410</td>\n",
       "      <td>0.321011</td>\n",
       "      <td>0.318078</td>\n",
       "      <td>0.641710</td>\n",
       "      <td>0.951932</td>\n",
       "      <td>1.170069</td>\n",
       "      <td>1.177711</td>\n",
       "      <td>0.987763</td>\n",
       "      <td>0.981345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.743405</td>\n",
       "      <td>0.916254</td>\n",
       "      <td>0.866453</td>\n",
       "      <td>0.953677</td>\n",
       "      <td>0.716259</td>\n",
       "      <td>0.692816</td>\n",
       "      <td>0.446713</td>\n",
       "      <td>0.539733</td>\n",
       "      <td>0.279293</td>\n",
       "      <td>0.180641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.886510</td>\n",
       "      <td>0.760716</td>\n",
       "      <td>0.751800</td>\n",
       "      <td>0.052198</td>\n",
       "      <td>-0.050958</td>\n",
       "      <td>-0.140734</td>\n",
       "      <td>-0.173480</td>\n",
       "      <td>0.178508</td>\n",
       "      <td>0.198187</td>\n",
       "      <td>0.357906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444142</td>\n",
       "      <td>0.492294</td>\n",
       "      <td>0.573348</td>\n",
       "      <td>0.546323</td>\n",
       "      <td>0.373874</td>\n",
       "      <td>0.699132</td>\n",
       "      <td>0.808303</td>\n",
       "      <td>1.118522</td>\n",
       "      <td>1.284887</td>\n",
       "      <td>1.541929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.187722</td>\n",
       "      <td>0.030199</td>\n",
       "      <td>-0.072558</td>\n",
       "      <td>-0.098400</td>\n",
       "      <td>-0.110795</td>\n",
       "      <td>-0.127632</td>\n",
       "      <td>-0.241193</td>\n",
       "      <td>-0.374608</td>\n",
       "      <td>-0.651771</td>\n",
       "      <td>-0.513491</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340927</td>\n",
       "      <td>-0.268253</td>\n",
       "      <td>-0.654777</td>\n",
       "      <td>-1.133722</td>\n",
       "      <td>-1.484557</td>\n",
       "      <td>-1.446644</td>\n",
       "      <td>-1.654337</td>\n",
       "      <td>-1.521009</td>\n",
       "      <td>-1.593825</td>\n",
       "      <td>-1.110684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.248822</td>\n",
       "      <td>0.168815</td>\n",
       "      <td>0.260804</td>\n",
       "      <td>0.505885</td>\n",
       "      <td>0.471486</td>\n",
       "      <td>1.018661</td>\n",
       "      <td>0.971406</td>\n",
       "      <td>1.062348</td>\n",
       "      <td>0.986871</td>\n",
       "      <td>0.947982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.422044</td>\n",
       "      <td>0.688196</td>\n",
       "      <td>0.382416</td>\n",
       "      <td>0.344843</td>\n",
       "      <td>0.177595</td>\n",
       "      <td>0.330549</td>\n",
       "      <td>0.595061</td>\n",
       "      <td>0.884860</td>\n",
       "      <td>1.125103</td>\n",
       "      <td>1.220779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3136</th>\n",
       "      <td>1.033977</td>\n",
       "      <td>1.043510</td>\n",
       "      <td>1.029088</td>\n",
       "      <td>1.038866</td>\n",
       "      <td>1.044243</td>\n",
       "      <td>1.032999</td>\n",
       "      <td>1.034221</td>\n",
       "      <td>1.027377</td>\n",
       "      <td>1.027377</td>\n",
       "      <td>1.005378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977756</td>\n",
       "      <td>0.944023</td>\n",
       "      <td>0.945001</td>\n",
       "      <td>0.938646</td>\n",
       "      <td>0.955268</td>\n",
       "      <td>0.945735</td>\n",
       "      <td>0.940112</td>\n",
       "      <td>0.943290</td>\n",
       "      <td>0.956245</td>\n",
       "      <td>0.947446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3137</th>\n",
       "      <td>0.998393</td>\n",
       "      <td>0.997933</td>\n",
       "      <td>0.989993</td>\n",
       "      <td>1.001384</td>\n",
       "      <td>0.995516</td>\n",
       "      <td>0.983090</td>\n",
       "      <td>0.989993</td>\n",
       "      <td>0.981479</td>\n",
       "      <td>0.993330</td>\n",
       "      <td>0.995679</td>\n",
       "      <td>...</td>\n",
       "      <td>1.017606</td>\n",
       "      <td>1.025608</td>\n",
       "      <td>1.027689</td>\n",
       "      <td>1.026088</td>\n",
       "      <td>1.026248</td>\n",
       "      <td>1.024808</td>\n",
       "      <td>1.022247</td>\n",
       "      <td>1.024488</td>\n",
       "      <td>1.025288</td>\n",
       "      <td>1.017286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3138</th>\n",
       "      <td>1.288652</td>\n",
       "      <td>1.279170</td>\n",
       "      <td>1.271384</td>\n",
       "      <td>1.280467</td>\n",
       "      <td>1.276575</td>\n",
       "      <td>1.245434</td>\n",
       "      <td>1.208504</td>\n",
       "      <td>1.207506</td>\n",
       "      <td>1.064278</td>\n",
       "      <td>1.063479</td>\n",
       "      <td>...</td>\n",
       "      <td>1.023056</td>\n",
       "      <td>1.022158</td>\n",
       "      <td>1.032638</td>\n",
       "      <td>1.026849</td>\n",
       "      <td>1.016369</td>\n",
       "      <td>1.014872</td>\n",
       "      <td>1.013774</td>\n",
       "      <td>1.023356</td>\n",
       "      <td>1.026450</td>\n",
       "      <td>1.028047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3139</th>\n",
       "      <td>0.804169</td>\n",
       "      <td>0.821925</td>\n",
       "      <td>0.824755</td>\n",
       "      <td>0.836336</td>\n",
       "      <td>0.840196</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.834277</td>\n",
       "      <td>0.833762</td>\n",
       "      <td>0.828101</td>\n",
       "      <td>0.832733</td>\n",
       "      <td>...</td>\n",
       "      <td>1.101904</td>\n",
       "      <td>1.101132</td>\n",
       "      <td>1.083119</td>\n",
       "      <td>1.113742</td>\n",
       "      <td>1.130468</td>\n",
       "      <td>1.122491</td>\n",
       "      <td>1.126608</td>\n",
       "      <td>1.134843</td>\n",
       "      <td>1.133042</td>\n",
       "      <td>1.113484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140</th>\n",
       "      <td>1.063461</td>\n",
       "      <td>1.063461</td>\n",
       "      <td>1.065692</td>\n",
       "      <td>1.081557</td>\n",
       "      <td>1.058999</td>\n",
       "      <td>1.056272</td>\n",
       "      <td>1.066931</td>\n",
       "      <td>1.062469</td>\n",
       "      <td>1.074368</td>\n",
       "      <td>1.090481</td>\n",
       "      <td>...</td>\n",
       "      <td>1.054041</td>\n",
       "      <td>1.080565</td>\n",
       "      <td>1.101884</td>\n",
       "      <td>1.103123</td>\n",
       "      <td>1.112048</td>\n",
       "      <td>1.088250</td>\n",
       "      <td>1.103867</td>\n",
       "      <td>1.108577</td>\n",
       "      <td>1.099653</td>\n",
       "      <td>1.099653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3141 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.670595 -0.839068 -0.734415 -0.587261 -0.788800 -0.975857 -0.774088   \n",
       "1     0.188165  0.166410  0.321011  0.318078  0.641710  0.951932  1.170069   \n",
       "2     0.886510  0.760716  0.751800  0.052198 -0.050958 -0.140734 -0.173480   \n",
       "3    -0.187722  0.030199 -0.072558 -0.098400 -0.110795 -0.127632 -0.241193   \n",
       "4     0.248822  0.168815  0.260804  0.505885  0.471486  1.018661  0.971406   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3136  1.033977  1.043510  1.029088  1.038866  1.044243  1.032999  1.034221   \n",
       "3137  0.998393  0.997933  0.989993  1.001384  0.995516  0.983090  0.989993   \n",
       "3138  1.288652  1.279170  1.271384  1.280467  1.276575  1.245434  1.208504   \n",
       "3139  0.804169  0.821925  0.824755  0.836336  0.840196  0.827586  0.834277   \n",
       "3140  1.063461  1.063461  1.065692  1.081557  1.058999  1.056272  1.066931   \n",
       "\n",
       "             7         8         9  ...        40        41        42  \\\n",
       "0    -1.021334 -1.110286 -0.893337  ... -0.997627 -1.103794 -1.092988   \n",
       "1     1.177711  0.987763  0.981345  ...  0.743405  0.916254  0.866453   \n",
       "2     0.178508  0.198187  0.357906  ...  0.444142  0.492294  0.573348   \n",
       "3    -0.374608 -0.651771 -0.513491  ... -0.340927 -0.268253 -0.654777   \n",
       "4     1.062348  0.986871  0.947982  ...  0.422044  0.688196  0.382416   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3136  1.027377  1.027377  1.005378  ...  0.977756  0.944023  0.945001   \n",
       "3137  0.981479  0.993330  0.995679  ...  1.017606  1.025608  1.027689   \n",
       "3138  1.207506  1.064278  1.063479  ...  1.023056  1.022158  1.032638   \n",
       "3139  0.833762  0.828101  0.832733  ...  1.101904  1.101132  1.083119   \n",
       "3140  1.062469  1.074368  1.090481  ...  1.054041  1.080565  1.101884   \n",
       "\n",
       "            43        44        45        46        47        48        49  \n",
       "0    -0.989165 -0.827528 -0.813729 -0.532411 -0.289483 -0.407720 -0.407505  \n",
       "1     0.953677  0.716259  0.692816  0.446713  0.539733  0.279293  0.180641  \n",
       "2     0.546323  0.373874  0.699132  0.808303  1.118522  1.284887  1.541929  \n",
       "3    -1.133722 -1.484557 -1.446644 -1.654337 -1.521009 -1.593825 -1.110684  \n",
       "4     0.344843  0.177595  0.330549  0.595061  0.884860  1.125103  1.220779  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3136  0.938646  0.955268  0.945735  0.940112  0.943290  0.956245  0.947446  \n",
       "3137  1.026088  1.026248  1.024808  1.022247  1.024488  1.025288  1.017286  \n",
       "3138  1.026849  1.016369  1.014872  1.013774  1.023356  1.026450  1.028047  \n",
       "3139  1.113742  1.130468  1.122491  1.126608  1.134843  1.133042  1.113484  \n",
       "3140  1.103123  1.112048  1.088250  1.103867  1.108577  1.099653  1.099653  \n",
       "\n",
       "[3141 rows x 50 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = train['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=['y','w'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.473380</td>\n",
       "      <td>1.302358</td>\n",
       "      <td>0.955648</td>\n",
       "      <td>0.507111</td>\n",
       "      <td>-0.010355</td>\n",
       "      <td>-0.393880</td>\n",
       "      <td>-0.530256</td>\n",
       "      <td>-0.823445</td>\n",
       "      <td>-0.765791</td>\n",
       "      <td>-0.164422</td>\n",
       "      <td>...</td>\n",
       "      <td>1.177141</td>\n",
       "      <td>1.395390</td>\n",
       "      <td>1.641736</td>\n",
       "      <td>1.273160</td>\n",
       "      <td>1.592917</td>\n",
       "      <td>1.673110</td>\n",
       "      <td>1.338512</td>\n",
       "      <td>1.305661</td>\n",
       "      <td>1.233640</td>\n",
       "      <td>0.772404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.424475</td>\n",
       "      <td>1.115568</td>\n",
       "      <td>0.739882</td>\n",
       "      <td>0.400290</td>\n",
       "      <td>0.279887</td>\n",
       "      <td>-0.132578</td>\n",
       "      <td>-0.211899</td>\n",
       "      <td>-0.428109</td>\n",
       "      <td>-0.437151</td>\n",
       "      <td>0.383196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673348</td>\n",
       "      <td>0.807094</td>\n",
       "      <td>1.004954</td>\n",
       "      <td>1.338058</td>\n",
       "      <td>1.201618</td>\n",
       "      <td>1.425135</td>\n",
       "      <td>1.400665</td>\n",
       "      <td>1.462675</td>\n",
       "      <td>1.349165</td>\n",
       "      <td>1.356235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.305604</td>\n",
       "      <td>1.178813</td>\n",
       "      <td>1.466944</td>\n",
       "      <td>1.360661</td>\n",
       "      <td>1.251295</td>\n",
       "      <td>0.914605</td>\n",
       "      <td>1.034285</td>\n",
       "      <td>0.899520</td>\n",
       "      <td>0.748029</td>\n",
       "      <td>0.661196</td>\n",
       "      <td>...</td>\n",
       "      <td>1.657412</td>\n",
       "      <td>1.623164</td>\n",
       "      <td>1.533968</td>\n",
       "      <td>1.265447</td>\n",
       "      <td>0.853404</td>\n",
       "      <td>0.580997</td>\n",
       "      <td>0.402628</td>\n",
       "      <td>0.431900</td>\n",
       "      <td>0.464468</td>\n",
       "      <td>0.691700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002807</td>\n",
       "      <td>-0.221519</td>\n",
       "      <td>-0.083365</td>\n",
       "      <td>-0.207559</td>\n",
       "      <td>-0.556045</td>\n",
       "      <td>-0.971370</td>\n",
       "      <td>-1.415740</td>\n",
       "      <td>-1.760730</td>\n",
       "      <td>-1.883560</td>\n",
       "      <td>-2.516417</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.542988</td>\n",
       "      <td>-0.501347</td>\n",
       "      <td>-0.818187</td>\n",
       "      <td>-0.913955</td>\n",
       "      <td>-1.140255</td>\n",
       "      <td>-1.420735</td>\n",
       "      <td>-1.883106</td>\n",
       "      <td>-1.811040</td>\n",
       "      <td>-2.196093</td>\n",
       "      <td>-1.704123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.849331</td>\n",
       "      <td>1.231393</td>\n",
       "      <td>0.801866</td>\n",
       "      <td>0.847607</td>\n",
       "      <td>0.670850</td>\n",
       "      <td>0.497142</td>\n",
       "      <td>0.205616</td>\n",
       "      <td>0.252102</td>\n",
       "      <td>0.391566</td>\n",
       "      <td>0.247023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.271387</td>\n",
       "      <td>-0.718060</td>\n",
       "      <td>-0.293903</td>\n",
       "      <td>-0.373091</td>\n",
       "      <td>-0.717006</td>\n",
       "      <td>-0.565019</td>\n",
       "      <td>-0.644409</td>\n",
       "      <td>-0.973549</td>\n",
       "      <td>-0.840768</td>\n",
       "      <td>-0.327801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7321</th>\n",
       "      <td>1.876208</td>\n",
       "      <td>1.883376</td>\n",
       "      <td>1.831151</td>\n",
       "      <td>1.836686</td>\n",
       "      <td>1.816896</td>\n",
       "      <td>1.803459</td>\n",
       "      <td>1.704366</td>\n",
       "      <td>1.566069</td>\n",
       "      <td>1.618514</td>\n",
       "      <td>1.600551</td>\n",
       "      <td>...</td>\n",
       "      <td>1.240904</td>\n",
       "      <td>1.232736</td>\n",
       "      <td>1.211619</td>\n",
       "      <td>1.213789</td>\n",
       "      <td>1.194198</td>\n",
       "      <td>1.177196</td>\n",
       "      <td>1.140504</td>\n",
       "      <td>1.110838</td>\n",
       "      <td>1.083161</td>\n",
       "      <td>1.053034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7322</th>\n",
       "      <td>1.999932</td>\n",
       "      <td>2.012557</td>\n",
       "      <td>1.977732</td>\n",
       "      <td>1.980653</td>\n",
       "      <td>1.961981</td>\n",
       "      <td>1.904541</td>\n",
       "      <td>1.897345</td>\n",
       "      <td>1.846402</td>\n",
       "      <td>1.821126</td>\n",
       "      <td>1.769551</td>\n",
       "      <td>...</td>\n",
       "      <td>1.264691</td>\n",
       "      <td>1.277377</td>\n",
       "      <td>1.274008</td>\n",
       "      <td>1.241438</td>\n",
       "      <td>1.220874</td>\n",
       "      <td>1.191414</td>\n",
       "      <td>1.167158</td>\n",
       "      <td>1.134478</td>\n",
       "      <td>1.104766</td>\n",
       "      <td>1.078077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7323</th>\n",
       "      <td>1.827227</td>\n",
       "      <td>1.827655</td>\n",
       "      <td>1.797092</td>\n",
       "      <td>1.777848</td>\n",
       "      <td>1.756158</td>\n",
       "      <td>1.714139</td>\n",
       "      <td>1.680463</td>\n",
       "      <td>1.630183</td>\n",
       "      <td>1.620579</td>\n",
       "      <td>1.606442</td>\n",
       "      <td>...</td>\n",
       "      <td>1.300441</td>\n",
       "      <td>1.306511</td>\n",
       "      <td>1.280204</td>\n",
       "      <td>1.266676</td>\n",
       "      <td>1.239488</td>\n",
       "      <td>1.213346</td>\n",
       "      <td>1.189792</td>\n",
       "      <td>1.165011</td>\n",
       "      <td>1.131039</td>\n",
       "      <td>1.105254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7324</th>\n",
       "      <td>1.589435</td>\n",
       "      <td>1.590090</td>\n",
       "      <td>1.605195</td>\n",
       "      <td>1.591595</td>\n",
       "      <td>1.581007</td>\n",
       "      <td>1.550989</td>\n",
       "      <td>1.531828</td>\n",
       "      <td>1.499694</td>\n",
       "      <td>1.481174</td>\n",
       "      <td>1.462716</td>\n",
       "      <td>...</td>\n",
       "      <td>1.290734</td>\n",
       "      <td>1.271182</td>\n",
       "      <td>1.242842</td>\n",
       "      <td>1.248176</td>\n",
       "      <td>1.216507</td>\n",
       "      <td>1.190271</td>\n",
       "      <td>1.169392</td>\n",
       "      <td>1.136811</td>\n",
       "      <td>1.110023</td>\n",
       "      <td>1.092333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7325</th>\n",
       "      <td>2.167003</td>\n",
       "      <td>2.168932</td>\n",
       "      <td>2.120908</td>\n",
       "      <td>2.113237</td>\n",
       "      <td>2.090573</td>\n",
       "      <td>2.073883</td>\n",
       "      <td>2.086083</td>\n",
       "      <td>2.011803</td>\n",
       "      <td>2.084486</td>\n",
       "      <td>1.792345</td>\n",
       "      <td>...</td>\n",
       "      <td>1.280456</td>\n",
       "      <td>1.271173</td>\n",
       "      <td>1.266993</td>\n",
       "      <td>1.253379</td>\n",
       "      <td>1.233420</td>\n",
       "      <td>1.210825</td>\n",
       "      <td>1.164077</td>\n",
       "      <td>1.132695</td>\n",
       "      <td>1.097706</td>\n",
       "      <td>1.058586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7326 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0     1.473380  1.302358  0.955648  0.507111 -0.010355 -0.393880 -0.530256   \n",
       "1     1.424475  1.115568  0.739882  0.400290  0.279887 -0.132578 -0.211899   \n",
       "2     1.305604  1.178813  1.466944  1.360661  1.251295  0.914605  1.034285   \n",
       "3     0.002807 -0.221519 -0.083365 -0.207559 -0.556045 -0.971370 -1.415740   \n",
       "4     0.849331  1.231393  0.801866  0.847607  0.670850  0.497142  0.205616   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7321  1.876208  1.883376  1.831151  1.836686  1.816896  1.803459  1.704366   \n",
       "7322  1.999932  2.012557  1.977732  1.980653  1.961981  1.904541  1.897345   \n",
       "7323  1.827227  1.827655  1.797092  1.777848  1.756158  1.714139  1.680463   \n",
       "7324  1.589435  1.590090  1.605195  1.591595  1.581007  1.550989  1.531828   \n",
       "7325  2.167003  2.168932  2.120908  2.113237  2.090573  2.073883  2.086083   \n",
       "\n",
       "             7         8         9  ...        40        41        42  \\\n",
       "0    -0.823445 -0.765791 -0.164422  ...  1.177141  1.395390  1.641736   \n",
       "1    -0.428109 -0.437151  0.383196  ...  0.673348  0.807094  1.004954   \n",
       "2     0.899520  0.748029  0.661196  ...  1.657412  1.623164  1.533968   \n",
       "3    -1.760730 -1.883560 -2.516417  ... -0.542988 -0.501347 -0.818187   \n",
       "4     0.252102  0.391566  0.247023  ... -0.271387 -0.718060 -0.293903   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7321  1.566069  1.618514  1.600551  ...  1.240904  1.232736  1.211619   \n",
       "7322  1.846402  1.821126  1.769551  ...  1.264691  1.277377  1.274008   \n",
       "7323  1.630183  1.620579  1.606442  ...  1.300441  1.306511  1.280204   \n",
       "7324  1.499694  1.481174  1.462716  ...  1.290734  1.271182  1.242842   \n",
       "7325  2.011803  2.084486  1.792345  ...  1.280456  1.271173  1.266993   \n",
       "\n",
       "            43        44        45        46        47        48        49  \n",
       "0     1.273160  1.592917  1.673110  1.338512  1.305661  1.233640  0.772404  \n",
       "1     1.338058  1.201618  1.425135  1.400665  1.462675  1.349165  1.356235  \n",
       "2     1.265447  0.853404  0.580997  0.402628  0.431900  0.464468  0.691700  \n",
       "3    -0.913955 -1.140255 -1.420735 -1.883106 -1.811040 -2.196093 -1.704123  \n",
       "4    -0.373091 -0.717006 -0.565019 -0.644409 -0.973549 -0.840768 -0.327801  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7321  1.213789  1.194198  1.177196  1.140504  1.110838  1.083161  1.053034  \n",
       "7322  1.241438  1.220874  1.191414  1.167158  1.134478  1.104766  1.078077  \n",
       "7323  1.266676  1.239488  1.213346  1.189792  1.165011  1.131039  1.105254  \n",
       "7324  1.248176  1.216507  1.190271  1.169392  1.136811  1.110023  1.092333  \n",
       "7325  1.253379  1.233420  1.210825  1.164077  1.132695  1.097706  1.058586  \n",
       "\n",
       "[7326 rows x 50 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler = MaxAbsScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# transform data\n",
    "cols = [f'{i}' for i in range (50)]\n",
    "train[cols] = scaler.fit_transform(train[cols])\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_and_targets = pd.concat([train, targets], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.613172\n",
       "1     0.618793\n",
       "2     0.621057\n",
       "3     0.614287\n",
       "4     0.602994\n",
       "5     0.589865\n",
       "6     0.584518\n",
       "7     0.580042\n",
       "8     0.579152\n",
       "9     0.588522\n",
       "10    0.601310\n",
       "11    0.612443\n",
       "12    0.626453\n",
       "13    0.642289\n",
       "14    0.657419\n",
       "15    0.666290\n",
       "16    0.673230\n",
       "17    0.677283\n",
       "18    0.681082\n",
       "19    0.679991\n",
       "20    0.683424\n",
       "21    0.688972\n",
       "22    0.701411\n",
       "23    0.715419\n",
       "24    0.726833\n",
       "25    0.745667\n",
       "26    0.765639\n",
       "27    0.783774\n",
       "28    0.802625\n",
       "29    0.819507\n",
       "30    0.828819\n",
       "31    0.831443\n",
       "32    0.826190\n",
       "33    0.808870\n",
       "34    0.779100\n",
       "35    0.743946\n",
       "36    0.709549\n",
       "37    0.683334\n",
       "38    0.664611\n",
       "39    0.656926\n",
       "40    0.658175\n",
       "41    0.670157\n",
       "42    0.684724\n",
       "43    0.708149\n",
       "44    0.745744\n",
       "45    0.790237\n",
       "46    0.843251\n",
       "47    0.898601\n",
       "48    0.944832\n",
       "49    0.977203\n",
       "y     1.000000\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = features_and_targets.corr()\n",
    "corr_matrix['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Lag', ylabel='Autocorrelation'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "autocorrelation_plot(train.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(features_and_targets,test_size=0.2,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_features = train_set.drop(columns=[\"y\"])\n",
    "train_set_features_no10 = train_set.drop(columns=[\"y\",'1','2','3','4','5','6','7','8','9'])\n",
    "train_set_targets = train_set[\"y\"]\n",
    "\n",
    "\n",
    "lin_reg = LinearRegression().fit(train_set_features_no10,train_set_targets)\n",
    "\n",
    "#len(train_set_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  0.023267055472198934\n",
      "Test_Random:  1.6203305688049923\n",
      "Train:  0.022756582213785237\n",
      "Train_Random:  1.6308344005477262\n"
     ]
    }
   ],
   "source": [
    "test_set_features_no10 = test_set.drop(columns=[\"y\",'1','2','3','4','5','6','7','8','9'])\n",
    "test_set_targets = test_set[\"y\"]\n",
    "\n",
    "train_predictions = lin_reg.predict(train_set_features_no10)\n",
    "test_predictions = lin_reg.predict(test_set_features_no10)\n",
    "\n",
    "# We can check what MSE we get for random predictions. \n",
    "# If this is not significantly worse than for our true predictions, something is wrong!\n",
    "\n",
    "from numpy.random import permutation\n",
    "\n",
    "print(\"Test: \", (mean_squared_error(test_set_targets,test_predictions)))\n",
    "print(\"Test_Random: \", (mean_squared_error(permutation(test_set_targets),test_predictions)))\n",
    "print(\"Train: \", (mean_squared_error(train_set_targets,train_predictions)))\n",
    "print(\"Train_Random: \", (mean_squared_error(permutation(train_set_targets),train_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoRegression model instead linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pietro/opt/anaconda3/envs/TF/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:579: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n",
      "  ' ignored when e.g. forecasting.', ValueWarning)\n",
      "/Users/pietro/opt/anaconda3/envs/TF/lib/python3.7/site-packages/statsmodels/tsa/ar_model.py:252: FutureWarning: The parameter names will change after 0.12 is released. Set old_names to False to use the new names now. Set old_names to True to use the old names. \n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "auto_reg_model = AutoReg(train_set_targets, exog = train_set_features_no10, lags=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pietro/opt/anaconda3/envs/TF/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:379: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  ValueWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "exog_oos must be provided when producing out-of-sample forecasts.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-849b3ac4ec99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mauto_reg_model_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauto_reg_model_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_features_no10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_features_no10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauto_reg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto_reg_model_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m51\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TF/lib/python3.7/site-packages/statsmodels/base/wrapper.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TF/lib/python3.7/site-packages/statsmodels/tsa/ar_model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, start, end, dynamic, exog, exog_oos)\u001b[0m\n\u001b[1;32m   2193\u001b[0m             \u001b[0mdynamic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdynamic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m             \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m             \u001b[0mexog_oos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexog_oos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2196\u001b[0m         )\n\u001b[1;32m   2197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/TF/lib/python3.7/site-packages/statsmodels/tsa/ar_model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, params, start, end, dynamic, exog, exog_oos)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnum_oos\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexog_oos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 739\u001b[0;31m                     \u001b[0;34m\"exog_oos must be provided when producing \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m                     \u001b[0;34m\"out-of-sample forecasts.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                 )\n",
      "\u001b[0;31mValueError\u001b[0m: exog_oos must be provided when producing out-of-sample forecasts."
     ]
    }
   ],
   "source": [
    "auto_reg_model_fit = auto_reg_model.fit()\n",
    "auto_reg_model_fit.summary()\n",
    "\n",
    "auto_reg_model_fit.params\n",
    "\n",
    "predictions = auto_reg_model_fit.predict(start=len(train_set_features_no10), end=len(train_set_features_no10)+1, dynamic=False)\n",
    "\n",
    "yhat = auto_reg_model.predict(auto_reg_model_fit, 50, 51)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Regressors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR()\n",
      "Test:  0.019056168661768364\n",
      "Test_Random:  1.6508153228203606\n",
      "Train:  0.015057813340880436\n",
      "Train_Random:  1.6254898327692369 \n",
      "\n",
      "SGDRegressor()\n",
      "Test:  0.024223590034411435\n",
      "Test_Random:  1.6460177064361237\n",
      "Train:  0.02342347670610448\n",
      "Train_Random:  1.6281675831737024 \n",
      "\n",
      "BayesianRidge()\n",
      "Test:  0.023279573315763722\n",
      "Test_Random:  1.6604993777621109\n",
      "Train:  0.022757322047504976\n",
      "Train_Random:  1.6467967998272477 \n",
      "\n",
      "LassoLars()\n",
      "Test:  0.8450073917030098\n",
      "Test_Random:  0.8450073917030095\n",
      "Train:  0.8294168195230618\n",
      "Train_Random:  0.8294168195230618 \n",
      "\n",
      "ARDRegression()\n",
      "Test:  0.023267097126798607\n",
      "Test_Random:  1.7226686832566303\n",
      "Train:  0.022793438537342836\n",
      "Train_Random:  1.6135528854918944 \n",
      "\n",
      "PassiveAggressiveRegressor()\n",
      "Test:  0.029889894387469\n",
      "Test_Random:  1.7312453453620082\n",
      "Train:  0.028960011350254663\n",
      "Train_Random:  1.6812847696896007 \n",
      "\n",
      "LinearRegression()\n",
      "Test:  0.023267055472198934\n",
      "Test_Random:  1.61860552091736\n",
      "Train:  0.022756582213785237\n",
      "Train_Random:  1.6437066195492953 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "\n",
    "classifiers = [\n",
    "    svm.SVR(),\n",
    "    linear_model.SGDRegressor(),\n",
    "    linear_model.BayesianRidge(),\n",
    "    linear_model.LassoLars(),\n",
    "    linear_model.ARDRegression(),\n",
    "    linear_model.PassiveAggressiveRegressor(),\n",
    "    linear_model.LinearRegression(),\n",
    "    #linear_model.TheilSenRegressor()\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    clf.fit(train_set_features_no10, train_set_targets)\n",
    "    ypredtrain = (clf.predict(train_set_features_no10))\n",
    "    ypredtest = (clf.predict(test_set_features_no10))\n",
    "    print(\"Test: \", (mean_squared_error(test_set_targets,ypredtest)))\n",
    "    print(\"Test_Random: \", (mean_squared_error(permutation(test_set_targets),ypredtest)))\n",
    "    print(\"Train: \", (mean_squared_error(train_set_targets,ypredtrain)))\n",
    "    print(\"Train_Random: \", (mean_squared_error(permutation(train_set_targets),ypredtrain)), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = linear_model.LinearRegression().fit(train, targets)\n",
    "\n",
    "ypredtest = (lin_reg.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('myfile.txt', np.c_[ypredtest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR()\n",
      "0.98 accuracy with a standard deviation of 0.00\n",
      "SGDRegressor()\n",
      "0.97 accuracy with a standard deviation of 0.00\n",
      "BayesianRidge()\n",
      "0.97 accuracy with a standard deviation of 0.00\n",
      "LassoLars()\n",
      "-0.00 accuracy with a standard deviation of 0.00\n",
      "ARDRegression()\n",
      "0.97 accuracy with a standard deviation of 0.00\n",
      "PassiveAggressiveRegressor()\n",
      "0.96 accuracy with a standard deviation of 0.01\n",
      "LinearRegression()\n",
      "0.97 accuracy with a standard deviation of 0.00\n"
     ]
    }
   ],
   "source": [
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    clf.fit(train_set_features_no10, train_set_targets)\n",
    "    scores = cross_val_score(clf, train_set_features_no10, train_set_targets, cv=5)\n",
    "    print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = svm.SVR().fit(train, targets)\n",
    "\n",
    "ypredtest = (lin_reg.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('myfile.txt', np.c_[ypredtest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyper-parameters\n"
     ]
    }
   ],
   "source": [
    "# Tuning of parameters for regression by cross-validation\n",
    "K = 5               # Number of cross valiations\n",
    "\n",
    "X = train_set_features \n",
    "y = train_set_targets\n",
    "\n",
    "# Parameters for tuning\n",
    "parameters = [{'kernel': ['rbf','linear','poly'], 'gamma': [1e-4, 1e-3, 0.01, 0.1, 0.2, 0.5, 0.6, 0.9],'C': [1, 10, 100, 1000, 10000]}]\n",
    "print(\"Tuning hyper-parameters\")\n",
    "svr = GridSearchCV(svm.SVR(epsilon = 0.01), parameters, cv = K)\n",
    "svr.fit(X, y)\n",
    "\n",
    "# Checking the score for all parameters\n",
    "print(\"Grid scores on training set:\")\n",
    "means = svr.cv_results_['mean_test_score']\n",
    "stds = svr.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, svr.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
